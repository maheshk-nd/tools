{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.quantization\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [Errno 111] Connection refused>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 14.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [Errno 111] Connection refused>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 464kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [Errno 111] Connection refused>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [Errno 111] Connection refused>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 18.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def accuracy(output, target):\n",
    "    \"\"\" Computes the top 1 accuracy \"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(1, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        correct_one = correct[:1].view(-1).float().sum(0, keepdim=True)\n",
    "        return correct_one.mul_(100.0 / batch_size).item()\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    \"\"\" Prints the real size of the model \"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def load_model(quantized_model, model):\n",
    "    \"\"\" Loads in the weights into an object meant for quantization \"\"\"\n",
    "    state_dict = model.state_dict()\n",
    "    model = model.to('cpu')\n",
    "    quantized_model.load_state_dict(state_dict)\n",
    "\n",
    "def fuse_modules(model):\n",
    "    \"\"\" Fuse together convolutions/linear layers and ReLU \"\"\"\n",
    "    torch.quantization.fuse_modules(model, [['conv1', 'relu1'], \n",
    "                                            ['conv2', 'relu2'],\n",
    "                                            ['fc1', 'relu3'],\n",
    "                                            ['fc2', 'relu4']], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, q = False):\n",
    "        # By turning on Q we can turn on/off the quantization\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256, 120, bias=False)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
    "        self.q = q\n",
    "        if q:\n",
    "          self.quant = QuantStub()\n",
    "          self.dequant = DeQuantStub()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.q:\n",
    "          x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        # Be careful to use reshape here instead of view\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc3(x)\n",
    "        if self.q:\n",
    "          x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.179057\n"
     ]
    }
   ],
   "source": [
    "net = Net(q=False).cuda()\n",
    "print_size_of_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, dataloader: DataLoader, cuda=False, q=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    model.train()\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = AverageMeter('loss')\n",
    "        acc = AverageMeter('train_acc')\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            if cuda:\n",
    "              inputs = inputs.cuda()\n",
    "              labels = labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if epoch>=3 and q:\n",
    "              model.apply(torch.quantization.disable_observer)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss.update(loss.item(), outputs.shape[0])\n",
    "            acc.update(accuracy(outputs, labels), outputs.shape[0])\n",
    "            if i % 100 == 0:    # print every 100 mini-batches\n",
    "                print('[%d, %5d] ' %\n",
    "                    (epoch + 1, i + 1), running_loss, acc)\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            if cuda:\n",
    "              inputs = inputs.cuda()\n",
    "              labels = labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1]  loss 2.301271 (2.301271) train_acc 4.687500 (4.687500)\n",
      "[1,   101]  loss 2.295300 (2.300438) train_acc 15.625000 (8.029084)\n",
      "[1,   201]  loss 2.292073 (2.298643) train_acc 21.875000 (10.820896)\n",
      "[1,   301]  loss 2.290231 (2.296418) train_acc 18.750000 (14.477782)\n",
      "[1,   401]  loss 2.276408 (2.293633) train_acc 28.125000 (17.471945)\n",
      "[1,   501]  loss 2.271868 (2.290192) train_acc 34.375000 (19.732410)\n",
      "[1,   601]  loss 2.249302 (2.285249) train_acc 37.500000 (22.665349)\n",
      "[1,   701]  loss 2.181720 (2.276491) train_acc 50.000000 (26.279422)\n",
      "[1,   801]  loss 1.985273 (2.257389) train_acc 71.875000 (29.993758)\n",
      "[1,   901]  loss 1.470307 (2.203546) train_acc 62.500000 (33.226970)\n",
      "[2,     1]  loss 1.074759 (1.074759) train_acc 73.437500 (73.437500)\n",
      "[2,   101]  loss 0.789274 (0.919006) train_acc 67.187500 (73.623144)\n",
      "[2,   201]  loss 0.682187 (0.777460) train_acc 82.812500 (76.951182)\n",
      "[2,   301]  loss 0.327191 (0.694045) train_acc 89.062500 (79.074958)\n",
      "[2,   401]  loss 0.458517 (0.630639) train_acc 89.062500 (80.848660)\n",
      "[2,   501]  loss 0.364872 (0.588383) train_acc 89.062500 (82.095185)\n",
      "[2,   601]  loss 0.335341 (0.549261) train_acc 90.625000 (83.306468)\n",
      "[2,   701]  loss 0.378939 (0.516274) train_acc 89.062500 (84.301444)\n",
      "[2,   801]  loss 0.317423 (0.486371) train_acc 89.062500 (85.243056)\n",
      "[2,   901]  loss 0.322569 (0.461970) train_acc 87.500000 (85.935766)\n",
      "[3,     1]  loss 0.362417 (0.362417) train_acc 85.937500 (85.937500)\n",
      "[3,   101]  loss 0.183781 (0.223261) train_acc 92.187500 (93.347772)\n",
      "[3,   201]  loss 0.123156 (0.223646) train_acc 93.750000 (93.198072)\n",
      "[3,   301]  loss 0.268020 (0.217312) train_acc 87.500000 (93.490449)\n",
      "[3,   401]  loss 0.204551 (0.210727) train_acc 93.750000 (93.652587)\n",
      "[3,   501]  loss 0.188919 (0.201608) train_acc 90.625000 (93.946482)\n",
      "[3,   601]  loss 0.113155 (0.197465) train_acc 98.437500 (94.082779)\n",
      "[3,   701]  loss 0.211625 (0.193421) train_acc 90.625000 (94.213623)\n",
      "[3,   801]  loss 0.156533 (0.190346) train_acc 95.312500 (94.235721)\n",
      "[3,   901]  loss 0.065566 (0.186189) train_acc 98.437500 (94.325749)\n",
      "[4,     1]  loss 0.079203 (0.079203) train_acc 98.437500 (98.437500)\n",
      "[4,   101]  loss 0.206080 (0.147194) train_acc 95.312500 (95.467203)\n",
      "[4,   201]  loss 0.043677 (0.145973) train_acc 100.000000 (95.553483)\n",
      "[4,   301]  loss 0.513314 (0.140674) train_acc 90.625000 (95.790075)\n",
      "[4,   401]  loss 0.108309 (0.140970) train_acc 95.312500 (95.752805)\n",
      "[4,   501]  loss 0.257692 (0.137098) train_acc 90.625000 (95.848927)\n",
      "[4,   601]  loss 0.141322 (0.134111) train_acc 93.750000 (95.928661)\n",
      "[4,   701]  loss 0.164235 (0.132108) train_acc 93.750000 (95.983417)\n",
      "[4,   801]  loss 0.186025 (0.130152) train_acc 95.312500 (96.081071)\n",
      "[4,   901]  loss 0.160301 (0.128121) train_acc 93.750000 (96.155314)\n",
      "[5,     1]  loss 0.070386 (0.070386) train_acc 96.875000 (96.875000)\n",
      "[5,   101]  loss 0.096013 (0.104736) train_acc 96.875000 (96.905941)\n",
      "[5,   201]  loss 0.104317 (0.104043) train_acc 96.875000 (96.921642)\n",
      "[5,   301]  loss 0.066073 (0.111839) train_acc 96.875000 (96.656977)\n",
      "[5,   401]  loss 0.152378 (0.111394) train_acc 93.750000 (96.668485)\n",
      "[5,   501]  loss 0.020961 (0.109366) train_acc 100.000000 (96.678518)\n",
      "[5,   601]  loss 0.137958 (0.105758) train_acc 93.750000 (96.817804)\n",
      "[5,   701]  loss 0.146716 (0.106104) train_acc 95.312500 (96.808131)\n",
      "[5,   801]  loss 0.145863 (0.104494) train_acc 96.875000 (96.837937)\n",
      "[5,   901]  loss 0.079438 (0.102902) train_acc 96.875000 (96.888873)\n",
      "[6,     1]  loss 0.196939 (0.196939) train_acc 92.187500 (92.187500)\n",
      "[6,   101]  loss 0.097381 (0.090830) train_acc 96.875000 (97.060644)\n",
      "[6,   201]  loss 0.015854 (0.095964) train_acc 100.000000 (96.976057)\n",
      "[6,   301]  loss 0.023765 (0.092701) train_acc 100.000000 (97.113787)\n",
      "[6,   401]  loss 0.150214 (0.093436) train_acc 95.312500 (97.155549)\n",
      "[6,   501]  loss 0.146000 (0.091854) train_acc 95.312500 (97.218064)\n",
      "[6,   601]  loss 0.102755 (0.089852) train_acc 96.875000 (97.283174)\n",
      "[6,   701]  loss 0.033626 (0.089180) train_acc 100.000000 (97.336394)\n",
      "[6,   801]  loss 0.134476 (0.088975) train_acc 93.750000 (97.335362)\n",
      "[6,   901]  loss 0.078648 (0.089309) train_acc 96.875000 (97.343230)\n",
      "[7,     1]  loss 0.076120 (0.076120) train_acc 98.437500 (98.437500)\n",
      "[7,   101]  loss 0.044237 (0.081478) train_acc 100.000000 (97.540223)\n",
      "[7,   201]  loss 0.104886 (0.083230) train_acc 98.437500 (97.613495)\n",
      "[7,   301]  loss 0.044275 (0.080928) train_acc 100.000000 (97.601744)\n",
      "[7,   401]  loss 0.020009 (0.080607) train_acc 100.000000 (97.576372)\n",
      "[7,   501]  loss 0.031728 (0.080163) train_acc 100.000000 (97.601672)\n",
      "[7,   601]  loss 0.060690 (0.079861) train_acc 98.437500 (97.602953)\n",
      "[7,   701]  loss 0.011666 (0.079886) train_acc 100.000000 (97.615014)\n",
      "[7,   801]  loss 0.080620 (0.080060) train_acc 96.875000 (97.606507)\n",
      "[7,   901]  loss 0.051117 (0.080388) train_acc 98.437500 (97.568674)\n",
      "[8,     1]  loss 0.054149 (0.054149) train_acc 96.875000 (96.875000)\n",
      "[8,   101]  loss 0.087860 (0.072725) train_acc 95.312500 (97.648515)\n",
      "[8,   201]  loss 0.020763 (0.071292) train_acc 100.000000 (97.737873)\n",
      "[8,   301]  loss 0.038653 (0.073122) train_acc 100.000000 (97.726329)\n",
      "[8,   401]  loss 0.107657 (0.071867) train_acc 96.875000 (97.755611)\n",
      "[8,   501]  loss 0.018913 (0.072003) train_acc 100.000000 (97.770085)\n",
      "[8,   601]  loss 0.056553 (0.072900) train_acc 98.437500 (97.745944)\n",
      "[8,   701]  loss 0.009069 (0.071968) train_acc 100.000000 (97.771041)\n",
      "[8,   801]  loss 0.090709 (0.071054) train_acc 95.312500 (97.813280)\n",
      "[8,   901]  loss 0.038738 (0.072116) train_acc 98.437500 (97.785447)\n",
      "[9,     1]  loss 0.031335 (0.031335) train_acc 100.000000 (100.000000)\n",
      "[9,   101]  loss 0.048650 (0.055799) train_acc 98.437500 (98.360149)\n",
      "[9,   201]  loss 0.082763 (0.069196) train_acc 95.312500 (97.885572)\n",
      "[9,   301]  loss 0.017613 (0.067442) train_acc 100.000000 (97.959925)\n",
      "[9,   401]  loss 0.140644 (0.067423) train_acc 95.312500 (97.942643)\n",
      "[9,   501]  loss 0.178728 (0.067306) train_acc 95.312500 (97.979042)\n",
      "[9,   601]  loss 0.017754 (0.068212) train_acc 100.000000 (97.964330)\n",
      "[9,   701]  loss 0.015051 (0.068087) train_acc 100.000000 (97.980563)\n",
      "[9,   801]  loss 0.095922 (0.067179) train_acc 96.875000 (98.012250)\n",
      "[9,   901]  loss 0.061293 (0.066588) train_acc 96.875000 (98.010891)\n",
      "[10,     1]  loss 0.067693 (0.067693) train_acc 98.437500 (98.437500)\n",
      "[10,   101]  loss 0.163968 (0.060854) train_acc 98.437500 (98.189975)\n",
      "[10,   201]  loss 0.083898 (0.062764) train_acc 98.437500 (98.056592)\n",
      "[10,   301]  loss 0.007504 (0.063298) train_acc 100.000000 (98.063746)\n",
      "[10,   401]  loss 0.026502 (0.062289) train_acc 98.437500 (98.090711)\n",
      "[10,   501]  loss 0.040851 (0.060925) train_acc 98.437500 (98.125624)\n",
      "[10,   601]  loss 0.041533 (0.062075) train_acc 98.437500 (98.096922)\n",
      "[10,   701]  loss 0.026211 (0.061579) train_acc 100.000000 (98.103156)\n",
      "[10,   801]  loss 0.052308 (0.061003) train_acc 98.437500 (98.117587)\n",
      "[10,   901]  loss 0.043685 (0.060861) train_acc 98.437500 (98.111473)\n",
      "[11,     1]  loss 0.012565 (0.012565) train_acc 100.000000 (100.000000)\n",
      "[11,   101]  loss 0.023189 (0.060680) train_acc 100.000000 (98.112624)\n",
      "[11,   201]  loss 0.042508 (0.058093) train_acc 98.437500 (98.180970)\n",
      "[11,   301]  loss 0.061431 (0.056357) train_acc 98.437500 (98.250623)\n",
      "[11,   401]  loss 0.016703 (0.056528) train_acc 100.000000 (98.277743)\n",
      "[11,   501]  loss 0.040679 (0.057617) train_acc 100.000000 (98.247255)\n",
      "[11,   601]  loss 0.087727 (0.055973) train_acc 95.312500 (98.286710)\n",
      "[11,   701]  loss 0.109000 (0.057049) train_acc 98.437500 (98.261412)\n",
      "[11,   801]  loss 0.033836 (0.057353) train_acc 100.000000 (98.250234)\n",
      "[11,   901]  loss 0.024448 (0.057505) train_acc 98.437500 (98.241537)\n",
      "[12,     1]  loss 0.014319 (0.014319) train_acc 100.000000 (100.000000)\n",
      "[12,   101]  loss 0.053079 (0.046230) train_acc 98.437500 (98.762376)\n",
      "[12,   201]  loss 0.144245 (0.048964) train_acc 98.437500 (98.616294)\n",
      "[12,   301]  loss 0.025833 (0.050040) train_acc 100.000000 (98.489410)\n",
      "[12,   401]  loss 0.045334 (0.050602) train_acc 98.437500 (98.468672)\n",
      "[12,   501]  loss 0.010147 (0.051500) train_acc 100.000000 (98.440619)\n",
      "[12,   601]  loss 0.007705 (0.051897) train_acc 100.000000 (98.395903)\n",
      "[12,   701]  loss 0.016055 (0.051675) train_acc 100.000000 (98.395150)\n",
      "[12,   801]  loss 0.064396 (0.052372) train_acc 98.437500 (98.378979)\n",
      "[12,   901]  loss 0.032018 (0.053037) train_acc 98.437500 (98.361196)\n",
      "[13,     1]  loss 0.034363 (0.034363) train_acc 98.437500 (98.437500)\n",
      "[13,   101]  loss 0.026613 (0.044422) train_acc 98.437500 (98.777847)\n",
      "[13,   201]  loss 0.020858 (0.046402) train_acc 100.000000 (98.701803)\n",
      "[13,   301]  loss 0.037004 (0.046497) train_acc 98.437500 (98.634759)\n",
      "[13,   401]  loss 0.018274 (0.047301) train_acc 100.000000 (98.608946)\n",
      "[13,   501]  loss 0.018944 (0.049276) train_acc 98.437500 (98.543538)\n",
      "[13,   601]  loss 0.026006 (0.050709) train_acc 100.000000 (98.484297)\n",
      "[13,   701]  loss 0.099804 (0.051456) train_acc 96.875000 (98.473163)\n",
      "[13,   801]  loss 0.013163 (0.051296) train_acc 100.000000 (98.451155)\n",
      "[13,   901]  loss 0.027322 (0.050882) train_acc 98.437500 (98.463513)\n",
      "[14,     1]  loss 0.172762 (0.172762) train_acc 96.875000 (96.875000)\n",
      "[14,   101]  loss 0.045369 (0.050064) train_acc 98.437500 (98.530322)\n",
      "[14,   201]  loss 0.007813 (0.048984) train_acc 100.000000 (98.538557)\n",
      "[14,   301]  loss 0.031491 (0.048300) train_acc 98.437500 (98.546512)\n",
      "[14,   401]  loss 0.012946 (0.047345) train_acc 100.000000 (98.511534)\n",
      "[14,   501]  loss 0.105698 (0.046428) train_acc 96.875000 (98.549775)\n",
      "[14,   601]  loss 0.042989 (0.046918) train_acc 98.437500 (98.544093)\n",
      "[14,   701]  loss 0.022164 (0.046824) train_acc 98.437500 (98.531116)\n",
      "[14,   801]  loss 0.052639 (0.047577) train_acc 98.437500 (98.517478)\n",
      "[14,   901]  loss 0.034692 (0.047862) train_acc 98.437500 (98.513804)\n",
      "[15,     1]  loss 0.024776 (0.024776) train_acc 100.000000 (100.000000)\n",
      "[15,   101]  loss 0.072392 (0.053382) train_acc 98.437500 (98.437500)\n",
      "[15,   201]  loss 0.059502 (0.049943) train_acc 96.875000 (98.507463)\n",
      "[15,   301]  loss 0.023047 (0.047362) train_acc 98.437500 (98.567276)\n",
      "[15,   401]  loss 0.119573 (0.044931) train_acc 96.875000 (98.632325)\n",
      "[15,   501]  loss 0.011417 (0.045161) train_acc 100.000000 (98.599676)\n",
      "[15,   601]  loss 0.058657 (0.044829) train_acc 98.437500 (98.619488)\n",
      "[15,   701]  loss 0.017861 (0.045079) train_acc 100.000000 (98.609130)\n",
      "[15,   801]  loss 0.003665 (0.045071) train_acc 100.000000 (98.601358)\n",
      "[15,   901]  loss 0.004033 (0.045194) train_acc 100.000000 (98.602248)\n",
      "[16,     1]  loss 0.079318 (0.079318) train_acc 96.875000 (96.875000)\n",
      "[16,   101]  loss 0.075196 (0.041777) train_acc 96.875000 (98.793317)\n",
      "[16,   201]  loss 0.023065 (0.040278) train_acc 98.437500 (98.709577)\n",
      "[16,   301]  loss 0.050629 (0.041427) train_acc 96.875000 (98.738580)\n",
      "[16,   401]  loss 0.045278 (0.041873) train_acc 98.437500 (98.745324)\n",
      "[16,   501]  loss 0.007780 (0.042249) train_acc 100.000000 (98.721307)\n",
      "[16,   601]  loss 0.041685 (0.042663) train_acc 98.437500 (98.726082)\n",
      "[16,   701]  loss 0.011016 (0.042650) train_acc 100.000000 (98.698288)\n",
      "[16,   801]  loss 0.019857 (0.042437) train_acc 100.000000 (98.700843)\n",
      "[16,   901]  loss 0.073435 (0.043062) train_acc 96.875000 (98.682020)\n",
      "[17,     1]  loss 0.018419 (0.018419) train_acc 100.000000 (100.000000)\n",
      "[17,   101]  loss 0.005064 (0.040625) train_acc 100.000000 (98.870668)\n",
      "[17,   201]  loss 0.036325 (0.041743) train_acc 98.437500 (98.787313)\n",
      "[17,   301]  loss 0.127539 (0.041647) train_acc 98.437500 (98.754153)\n",
      "[17,   401]  loss 0.181793 (0.040217) train_acc 98.437500 (98.780393)\n",
      "[17,   501]  loss 0.056639 (0.040437) train_acc 98.437500 (98.780564)\n",
      "[17,   601]  loss 0.046717 (0.040539) train_acc 98.437500 (98.778078)\n",
      "[17,   701]  loss 0.015937 (0.040066) train_acc 100.000000 (98.800820)\n",
      "[17,   801]  loss 0.017440 (0.040301) train_acc 100.000000 (98.788624)\n",
      "[17,   901]  loss 0.084785 (0.041204) train_acc 98.437500 (98.751387)\n",
      "[18,     1]  loss 0.039113 (0.039113) train_acc 96.875000 (96.875000)\n",
      "[18,   101]  loss 0.089221 (0.034918) train_acc 96.875000 (98.932550)\n",
      "[18,   201]  loss 0.049251 (0.040750) train_acc 98.437500 (98.818408)\n",
      "[18,   301]  loss 0.019722 (0.039198) train_acc 100.000000 (98.852782)\n",
      "[18,   401]  loss 0.042599 (0.038236) train_acc 98.437500 (98.912874)\n",
      "[18,   501]  loss 0.035398 (0.037191) train_acc 98.437500 (98.914671)\n",
      "[18,   601]  loss 0.012129 (0.037874) train_acc 100.000000 (98.892471)\n",
      "[18,   701]  loss 0.008504 (0.038842) train_acc 100.000000 (98.856544)\n",
      "[18,   801]  loss 0.050175 (0.039351) train_acc 96.875000 (98.804229)\n",
      "[18,   901]  loss 0.037539 (0.038983) train_acc 98.437500 (98.810350)\n",
      "[19,     1]  loss 0.093884 (0.093884) train_acc 95.312500 (95.312500)\n",
      "[19,   101]  loss 0.083319 (0.035397) train_acc 95.312500 (98.700495)\n",
      "[19,   201]  loss 0.033729 (0.037202) train_acc 100.000000 (98.810634)\n",
      "[19,   301]  loss 0.003550 (0.037481) train_acc 100.000000 (98.795681)\n",
      "[19,   401]  loss 0.022987 (0.036096) train_acc 98.437500 (98.842737)\n",
      "[19,   501]  loss 0.019859 (0.037243) train_acc 100.000000 (98.811751)\n",
      "[19,   601]  loss 0.014130 (0.037098) train_acc 100.000000 (98.819676)\n",
      "[19,   701]  loss 0.036227 (0.036727) train_acc 98.437500 (98.838713)\n",
      "[19,   801]  loss 0.055019 (0.036429) train_acc 98.437500 (98.852996)\n",
      "[19,   901]  loss 0.053183 (0.037067) train_acc 98.437500 (98.848502)\n",
      "[20,     1]  loss 0.014568 (0.014568) train_acc 100.000000 (100.000000)\n",
      "[20,   101]  loss 0.048845 (0.032829) train_acc 96.875000 (98.808787)\n",
      "[20,   201]  loss 0.039974 (0.032393) train_acc 98.437500 (98.872823)\n",
      "[20,   301]  loss 0.065035 (0.034213) train_acc 95.312500 (98.821636)\n",
      "[20,   401]  loss 0.013418 (0.036188) train_acc 100.000000 (98.815461)\n",
      "[20,   501]  loss 0.021451 (0.035733) train_acc 100.000000 (98.855414)\n",
      "[20,   601]  loss 0.008382 (0.034624) train_acc 100.000000 (98.905470)\n",
      "[20,   701]  loss 0.019729 (0.034626) train_acc 98.437500 (98.945703)\n",
      "[20,   801]  loss 0.065741 (0.035416) train_acc 96.875000 (98.921270)\n",
      "[20,   901]  loss 0.017465 (0.035309) train_acc 100.000000 (98.916135)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, trainloader, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.74% - FP32\n"
     ]
    }
   ],
   "source": [
    "score = test(net, testloader, cuda=True)\n",
    "print('Accuracy of the network on the test images: {}% - FP32'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Net(q=True)\n",
    "load_model(qnet, net)\n",
    "fuse_modules(qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.179249\n",
      "Accuracy of the fused network on the test images: 98.74% - FP32\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(qnet)\n",
    "score = test(qnet, testloader, cuda=False)\n",
    "print('Accuracy of the fused network on the test images: {}% - FP32'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Conv1: After observer insertion \n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      ")\n",
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " Conv1: After fusion and quantization \n",
      "\n",
      " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.052600838243961334, zero_point=0, bias=False)\n",
      "Size of model after quantization\n",
      "Size (MB): 0.050084\n"
     ]
    }
   ],
   "source": [
    "qnet.qconfig = torch.quantization.default_qconfig\n",
    "print(qnet.qconfig)\n",
    "torch.quantization.prepare(qnet, inplace=True)\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n Conv1: After observer insertion \\n\\n', qnet.conv1)\n",
    "\n",
    "test(qnet, trainloader, cuda=False)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "torch.quantization.convert(qnet, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fused and quantized network on the test images: 98.76% - INT8\n"
     ]
    }
   ],
   "source": [
    "score = test(qnet, testloader, cuda=False)\n",
    "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Conv1: After observer insertion \n",
      "\n",
      " ConvReLU2d(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (1): ReLU()\n",
      "  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " Conv1: After fusion and quantization \n",
      "\n",
      " QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.049229469150304794, zero_point=0, bias=False)\n",
      "Size of model after quantization\n",
      "Size (MB): 0.050084\n",
      "Accuracy of the fused and quantized network on the test images: 98.78% - INT8\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.observer import MovingAverageMinMaxObserver\n",
    "\n",
    "qnet = Net(q=True)\n",
    "load_model(qnet, net)\n",
    "fuse_modules(qnet)\n",
    "\n",
    "qnet.qconfig = torch.quantization.QConfig(\n",
    "                                      activation=MovingAverageMinMaxObserver.with_args(reduce_range=True), \n",
    "                                      weight=MovingAverageMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
    "print(qnet.qconfig)\n",
    "torch.quantization.prepare(qnet, inplace=True)\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n Conv1: After observer insertion \\n\\n', qnet.conv1)\n",
    "\n",
    "test(qnet, trainloader, cuda=False)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "torch.quantization.convert(qnet, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(qnet)\n",
    "score = test(qnet, testloader, cuda=False)\n",
    "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = Net(q=True)\n",
    "load_model(qnet, net)\n",
    "fuse_modules(qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Size of model after quantization\n",
      "Size (MB): 0.05572\n"
     ]
    }
   ],
   "source": [
    "qnet.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(qnet.qconfig)\n",
    "\n",
    "torch.quantization.prepare(qnet, inplace=True)\n",
    "test(qnet, trainloader, cuda=False)\n",
    "torch.quantization.convert(qnet, inplace=True)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(qnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the fused and quantized network on the test images: 98.7% - INT8\n"
     ]
    }
   ],
   "source": [
    "score = test(qnet, testloader, cuda=False)\n",
    "print('Accuracy of the fused and quantized network on the test images: {}% - INT8'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Conv1: After fusion and quantization \n",
      "\n",
      " ConvReLU2d(\n",
      "  1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False\n",
      "  (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "    (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "  )\n",
      "  (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "    fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "    (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qnet = Net(q=True)\n",
    "fuse_modules(qnet)\n",
    "qnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(qnet, inplace=True)\n",
    "print('\\n Conv1: After fusion and quantization \\n\\n', qnet.conv1)\n",
    "qnet=qnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1]  loss 2.304098 (2.304098) train_acc 14.062500 (14.062500)\n",
      "[1,   101]  loss 2.299604 (2.301910) train_acc 14.062500 (11.834777)\n",
      "[1,   201]  loss 2.296406 (2.299836) train_acc 17.187500 (13.456157)\n",
      "[1,   301]  loss 2.291352 (2.297608) train_acc 20.312500 (15.484842)\n",
      "[1,   401]  loss 2.288203 (2.294956) train_acc 25.000000 (17.740804)\n",
      "[1,   501]  loss 2.280631 (2.291599) train_acc 18.750000 (20.602545)\n",
      "[1,   601]  loss 2.257500 (2.286950) train_acc 46.875000 (24.178453)\n",
      "[1,   701]  loss 2.221570 (2.279951) train_acc 57.812500 (28.203014)\n",
      "[1,   801]  loss 2.159049 (2.268833) train_acc 57.812500 (32.020521)\n",
      "[1,   901]  loss 2.048853 (2.251091) train_acc 59.375000 (35.328801)\n",
      "[2,     1]  loss 2.010422 (2.010422) train_acc 54.687500 (54.687500)\n",
      "[2,   101]  loss 1.732704 (1.872161) train_acc 64.062500 (67.651609)\n",
      "[2,   201]  loss 1.267590 (1.712890) train_acc 79.687500 (69.986007)\n",
      "[2,   301]  loss 1.061672 (1.520588) train_acc 68.750000 (72.030731)\n",
      "[2,   401]  loss 0.729978 (1.344084) train_acc 71.875000 (73.706359)\n",
      "[2,   501]  loss 0.614777 (1.199407) train_acc 79.687500 (75.399202)\n",
      "[2,   601]  loss 0.356935 (1.090412) train_acc 90.625000 (76.708091)\n",
      "[2,   701]  loss 0.452075 (1.001992) train_acc 84.375000 (78.009094)\n",
      "[2,   801]  loss 0.556174 (0.931904) train_acc 81.250000 (79.065231)\n",
      "[2,   901]  loss 0.218834 (0.869840) train_acc 96.875000 (80.166135)\n",
      "[3,     1]  loss 0.416938 (0.416938) train_acc 87.500000 (87.500000)\n",
      "[3,   101]  loss 0.331425 (0.343107) train_acc 89.062500 (89.619431)\n",
      "[3,   201]  loss 0.285926 (0.326352) train_acc 92.187500 (90.073072)\n",
      "[3,   301]  loss 0.392616 (0.322232) train_acc 90.625000 (90.116279)\n",
      "[3,   401]  loss 0.441332 (0.310130) train_acc 87.500000 (90.445761)\n",
      "[3,   501]  loss 0.351966 (0.299031) train_acc 87.500000 (90.681138)\n",
      "[3,   601]  loss 0.194243 (0.290248) train_acc 93.750000 (90.996776)\n",
      "[3,   701]  loss 0.183066 (0.281083) train_acc 93.750000 (91.329351)\n",
      "[3,   801]  loss 0.153354 (0.273710) train_acc 93.750000 (91.518414)\n",
      "[3,   901]  loss 0.173088 (0.266412) train_acc 95.312500 (91.753954)\n",
      "[4,     1]  loss 0.217586 (0.217586) train_acc 96.875000 (96.875000)\n",
      "[4,   101]  loss 0.156807 (0.182641) train_acc 96.875000 (94.492574)\n",
      "[4,   201]  loss 0.322318 (0.185261) train_acc 92.187500 (94.410759)\n",
      "[4,   301]  loss 0.126727 (0.180524) train_acc 95.312500 (94.466362)\n",
      "[4,   401]  loss 0.083645 (0.179087) train_acc 98.437500 (94.537095)\n",
      "[4,   501]  loss 0.109393 (0.176366) train_acc 96.875000 (94.563997)\n",
      "[4,   601]  loss 0.100189 (0.174446) train_acc 98.437500 (94.607945)\n",
      "[4,   701]  loss 0.325592 (0.172598) train_acc 90.625000 (94.688392)\n",
      "[4,   801]  loss 0.104934 (0.169319) train_acc 96.875000 (94.764357)\n",
      "[4,   901]  loss 0.105696 (0.167027) train_acc 98.437500 (94.861612)\n",
      "[5,     1]  loss 0.148439 (0.148439) train_acc 95.312500 (95.312500)\n",
      "[5,   101]  loss 0.101473 (0.139687) train_acc 98.437500 (95.699257)\n",
      "[5,   201]  loss 0.031851 (0.133930) train_acc 100.000000 (95.864428)\n",
      "[5,   301]  loss 0.200983 (0.133382) train_acc 93.750000 (95.888704)\n",
      "[5,   401]  loss 0.132182 (0.132705) train_acc 98.437500 (95.869701)\n",
      "[5,   501]  loss 0.063786 (0.133860) train_acc 98.437500 (95.864521)\n",
      "[5,   601]  loss 0.083638 (0.132836) train_acc 98.437500 (95.923461)\n",
      "[5,   701]  loss 0.121821 (0.129985) train_acc 93.750000 (96.007935)\n",
      "[5,   801]  loss 0.080528 (0.128678) train_acc 98.437500 (96.044007)\n",
      "[5,   901]  loss 0.102618 (0.128115) train_acc 96.875000 (96.063402)\n",
      "[6,     1]  loss 0.060684 (0.060684) train_acc 96.875000 (96.875000)\n",
      "[6,   101]  loss 0.094891 (0.126907) train_acc 96.875000 (96.364480)\n",
      "[6,   201]  loss 0.045629 (0.120886) train_acc 98.437500 (96.385261)\n",
      "[6,   301]  loss 0.057939 (0.114041) train_acc 98.437500 (96.599875)\n",
      "[6,   401]  loss 0.056994 (0.114291) train_acc 98.437500 (96.547693)\n",
      "[6,   501]  loss 0.081992 (0.110901) train_acc 96.875000 (96.647330)\n",
      "[6,   601]  loss 0.118682 (0.107883) train_acc 96.875000 (96.724210)\n",
      "[6,   701]  loss 0.064381 (0.107116) train_acc 96.875000 (96.736805)\n",
      "[6,   801]  loss 0.061210 (0.105800) train_acc 98.437500 (96.767712)\n",
      "[6,   901]  loss 0.119650 (0.105077) train_acc 95.312500 (96.765746)\n",
      "[7,     1]  loss 0.068036 (0.068036) train_acc 98.437500 (98.437500)\n",
      "[7,   101]  loss 0.134049 (0.106952) train_acc 98.437500 (97.045173)\n",
      "[7,   201]  loss 0.116743 (0.098630) train_acc 95.312500 (97.217040)\n",
      "[7,   301]  loss 0.164023 (0.092729) train_acc 93.750000 (97.357766)\n",
      "[7,   401]  loss 0.092265 (0.091214) train_acc 96.875000 (97.303616)\n",
      "[7,   501]  loss 0.077124 (0.090913) train_acc 98.437500 (97.305389)\n",
      "[7,   601]  loss 0.082717 (0.090501) train_acc 98.437500 (97.283174)\n",
      "[7,   701]  loss 0.023024 (0.090497) train_acc 100.000000 (97.262839)\n",
      "[7,   801]  loss 0.040567 (0.090571) train_acc 98.437500 (97.257335)\n",
      "[7,   901]  loss 0.140263 (0.090157) train_acc 96.875000 (97.280799)\n",
      "[8,     1]  loss 0.050900 (0.050900) train_acc 98.437500 (98.437500)\n",
      "[8,   101]  loss 0.019209 (0.083043) train_acc 100.000000 (97.509282)\n",
      "[8,   201]  loss 0.033670 (0.083818) train_acc 100.000000 (97.489117)\n",
      "[8,   301]  loss 0.156396 (0.085292) train_acc 95.312500 (97.388912)\n",
      "[8,   401]  loss 0.121128 (0.084789) train_acc 96.875000 (97.385443)\n",
      "[8,   501]  loss 0.189005 (0.084586) train_acc 93.750000 (97.377121)\n",
      "[8,   601]  loss 0.029255 (0.082467) train_acc 100.000000 (97.431364)\n",
      "[8,   701]  loss 0.085362 (0.082693) train_acc 95.312500 (97.394347)\n",
      "[8,   801]  loss 0.034903 (0.081242) train_acc 100.000000 (97.450453)\n",
      "[8,   901]  loss 0.029428 (0.080832) train_acc 98.437500 (97.457686)\n",
      "[9,     1]  loss 0.032462 (0.032462) train_acc 100.000000 (100.000000)\n",
      "[9,   101]  loss 0.081538 (0.075667) train_acc 98.437500 (97.555693)\n",
      "[9,   201]  loss 0.072304 (0.079415) train_acc 98.437500 (97.551306)\n",
      "[9,   301]  loss 0.122384 (0.080060) train_acc 93.750000 (97.596553)\n",
      "[9,   401]  loss 0.028341 (0.079837) train_acc 100.000000 (97.603647)\n",
      "[9,   501]  loss 0.086427 (0.079028) train_acc 96.875000 (97.604790)\n",
      "[9,   601]  loss 0.024180 (0.077208) train_acc 100.000000 (97.623752)\n",
      "[9,   701]  loss 0.008923 (0.075409) train_acc 100.000000 (97.661822)\n",
      "[9,   801]  loss 0.034463 (0.074505) train_acc 98.437500 (97.672831)\n",
      "[9,   901]  loss 0.023895 (0.072979) train_acc 100.000000 (97.729953)\n",
      "[10,     1]  loss 0.047992 (0.047992) train_acc 98.437500 (98.437500)\n",
      "[10,   101]  loss 0.041228 (0.069881) train_acc 98.437500 (97.834158)\n",
      "[10,   201]  loss 0.051789 (0.067474) train_acc 98.437500 (97.901119)\n",
      "[10,   301]  loss 0.086513 (0.066602) train_acc 98.437500 (97.959925)\n",
      "[10,   401]  loss 0.048294 (0.067076) train_acc 98.437500 (97.927057)\n",
      "[10,   501]  loss 0.067020 (0.068320) train_acc 98.437500 (97.916667)\n",
      "[10,   601]  loss 0.035680 (0.066626) train_acc 100.000000 (97.964330)\n",
      "[10,   701]  loss 0.122479 (0.066398) train_acc 96.875000 (97.953816)\n",
      "[10,   801]  loss 0.177782 (0.066458) train_acc 93.750000 (97.936174)\n",
      "[10,   901]  loss 0.052986 (0.066928) train_acc 98.437500 (97.943257)\n",
      "[11,     1]  loss 0.085213 (0.085213) train_acc 96.875000 (96.875000)\n",
      "[11,   101]  loss 0.052914 (0.067132) train_acc 98.437500 (98.050743)\n",
      "[11,   201]  loss 0.053638 (0.064569) train_acc 98.437500 (98.134328)\n",
      "[11,   301]  loss 0.078345 (0.066357) train_acc 98.437500 (98.042982)\n",
      "[11,   401]  loss 0.024952 (0.064259) train_acc 100.000000 (98.067332)\n",
      "[11,   501]  loss 0.050867 (0.063087) train_acc 98.437500 (98.072605)\n",
      "[11,   601]  loss 0.054150 (0.062753) train_acc 98.437500 (98.060524)\n",
      "[11,   701]  loss 0.106921 (0.062371) train_acc 98.437500 (98.056348)\n",
      "[11,   801]  loss 0.091128 (0.061548) train_acc 96.875000 (98.092228)\n",
      "[11,   901]  loss 0.077729 (0.061069) train_acc 98.437500 (98.095866)\n",
      "[12,     1]  loss 0.045678 (0.045678) train_acc 98.437500 (98.437500)\n",
      "[12,   101]  loss 0.050099 (0.055837) train_acc 98.437500 (98.298267)\n",
      "[12,   201]  loss 0.039211 (0.059670) train_acc 98.437500 (98.243159)\n",
      "[12,   301]  loss 0.146990 (0.062172) train_acc 95.312500 (98.177949)\n",
      "[12,   401]  loss 0.054742 (0.059550) train_acc 98.437500 (98.238778)\n",
      "[12,   501]  loss 0.138574 (0.059076) train_acc 96.875000 (98.200474)\n",
      "[12,   601]  loss 0.069175 (0.059224) train_acc 98.437500 (98.174917)\n",
      "[12,   701]  loss 0.126060 (0.058749) train_acc 93.750000 (98.170025)\n",
      "[12,   801]  loss 0.046931 (0.057368) train_acc 96.875000 (98.228777)\n",
      "[12,   901]  loss 0.070738 (0.057151) train_acc 98.437500 (98.239803)\n",
      "[13,     1]  loss 0.177401 (0.177401) train_acc 95.312500 (95.312500)\n",
      "[13,   101]  loss 0.141142 (0.046121) train_acc 96.875000 (98.468441)\n",
      "[13,   201]  loss 0.027423 (0.049844) train_acc 98.437500 (98.406405)\n",
      "[13,   301]  loss 0.007360 (0.052577) train_acc 100.000000 (98.338870)\n",
      "[13,   401]  loss 0.048825 (0.052270) train_acc 98.437500 (98.375156)\n",
      "[13,   501]  loss 0.114652 (0.052492) train_acc 98.437500 (98.387600)\n",
      "[13,   601]  loss 0.020211 (0.052163) train_acc 98.437500 (98.385503)\n",
      "[13,   701]  loss 0.021905 (0.052120) train_acc 100.000000 (98.357257)\n",
      "[13,   801]  loss 0.027148 (0.051552) train_acc 100.000000 (98.373127)\n",
      "[13,   901]  loss 0.164605 (0.052129) train_acc 95.312500 (98.369867)\n",
      "[14,     1]  loss 0.014741 (0.014741) train_acc 100.000000 (100.000000)\n",
      "[14,   101]  loss 0.069656 (0.040728) train_acc 96.875000 (98.839728)\n",
      "[14,   201]  loss 0.059597 (0.043629) train_acc 98.437500 (98.740672)\n",
      "[14,   301]  loss 0.114268 (0.045899) train_acc 96.875000 (98.619186)\n",
      "[14,   401]  loss 0.027484 (0.048759) train_acc 100.000000 (98.519327)\n",
      "[14,   501]  loss 0.100791 (0.049870) train_acc 96.875000 (98.440619)\n",
      "[14,   601]  loss 0.149322 (0.050290) train_acc 98.437500 (98.403702)\n",
      "[14,   701]  loss 0.105052 (0.049944) train_acc 98.437500 (98.397379)\n",
      "[14,   801]  loss 0.036481 (0.050150) train_acc 98.437500 (98.410190)\n",
      "[14,   901]  loss 0.050497 (0.049415) train_acc 98.437500 (98.440968)\n",
      "[15,     1]  loss 0.084794 (0.084794) train_acc 98.437500 (98.437500)\n",
      "[15,   101]  loss 0.072887 (0.051810) train_acc 98.437500 (98.468441)\n",
      "[15,   201]  loss 0.034186 (0.049019) train_acc 98.437500 (98.530784)\n",
      "[15,   301]  loss 0.007475 (0.047125) train_acc 100.000000 (98.510174)\n",
      "[15,   401]  loss 0.011617 (0.047290) train_acc 100.000000 (98.527120)\n",
      "[15,   501]  loss 0.072165 (0.046941) train_acc 98.437500 (98.543538)\n",
      "[15,   601]  loss 0.013579 (0.046343) train_acc 100.000000 (98.544093)\n",
      "[15,   701]  loss 0.106879 (0.046515) train_acc 96.875000 (98.540032)\n",
      "[15,   801]  loss 0.013310 (0.047429) train_acc 100.000000 (98.507725)\n",
      "[15,   901]  loss 0.074521 (0.046957) train_acc 98.437500 (98.525943)\n",
      "[16,     1]  loss 0.102602 (0.102602) train_acc 98.437500 (98.437500)\n",
      "[16,   101]  loss 0.012044 (0.042741) train_acc 100.000000 (98.731436)\n",
      "[16,   201]  loss 0.066773 (0.044369) train_acc 98.437500 (98.624067)\n",
      "[16,   301]  loss 0.090614 (0.043520) train_acc 98.437500 (98.634759)\n",
      "[16,   401]  loss 0.068182 (0.042970) train_acc 98.437500 (98.690773)\n",
      "[16,   501]  loss 0.088651 (0.042965) train_acc 98.437500 (98.652695)\n",
      "[16,   601]  loss 0.053505 (0.045047) train_acc 98.437500 (98.585691)\n",
      "[16,   701]  loss 0.093980 (0.044513) train_acc 95.312500 (98.593527)\n",
      "[16,   801]  loss 0.028249 (0.044662) train_acc 98.437500 (98.597456)\n",
      "[16,   901]  loss 0.014440 (0.044820) train_acc 100.000000 (98.602248)\n",
      "[17,     1]  loss 0.040854 (0.040854) train_acc 96.875000 (96.875000)\n",
      "[17,   101]  loss 0.035293 (0.041024) train_acc 98.437500 (98.607673)\n",
      "[17,   201]  loss 0.027591 (0.038809) train_acc 98.437500 (98.740672)\n",
      "[17,   301]  loss 0.014350 (0.038201) train_acc 100.000000 (98.857973)\n",
      "[17,   401]  loss 0.077381 (0.039705) train_acc 96.875000 (98.784289)\n",
      "[17,   501]  loss 0.031179 (0.040435) train_acc 98.437500 (98.777445)\n",
      "[17,   601]  loss 0.008798 (0.040488) train_acc 100.000000 (98.765079)\n",
      "[17,   701]  loss 0.026199 (0.039893) train_acc 98.437500 (98.762928)\n",
      "[17,   801]  loss 0.016459 (0.040872) train_acc 100.000000 (98.749610)\n",
      "[17,   901]  loss 0.036848 (0.041215) train_acc 98.437500 (98.739248)\n",
      "[18,     1]  loss 0.004449 (0.004449) train_acc 100.000000 (100.000000)\n",
      "[18,   101]  loss 0.012143 (0.038909) train_acc 100.000000 (98.700495)\n",
      "[18,   201]  loss 0.021319 (0.040221) train_acc 98.437500 (98.639614)\n",
      "[18,   301]  loss 0.035224 (0.043455) train_acc 98.437500 (98.556894)\n",
      "[18,   401]  loss 0.003904 (0.043330) train_acc 100.000000 (98.624532)\n",
      "[18,   501]  loss 0.033968 (0.041806) train_acc 98.437500 (98.662051)\n",
      "[18,   601]  loss 0.014916 (0.039809) train_acc 100.000000 (98.744280)\n",
      "[18,   701]  loss 0.088535 (0.039072) train_acc 96.875000 (98.800820)\n",
      "[18,   801]  loss 0.139658 (0.039470) train_acc 95.312500 (98.780821)\n",
      "[18,   901]  loss 0.050387 (0.040176) train_acc 98.437500 (98.773932)\n",
      "[19,     1]  loss 0.102541 (0.102541) train_acc 98.437500 (98.437500)\n",
      "[19,   101]  loss 0.078510 (0.037620) train_acc 96.875000 (98.917079)\n",
      "[19,   201]  loss 0.043113 (0.040124) train_acc 98.437500 (98.802861)\n",
      "[19,   301]  loss 0.021967 (0.041208) train_acc 100.000000 (98.764535)\n",
      "[19,   401]  loss 0.018230 (0.037747) train_acc 100.000000 (98.846633)\n",
      "[19,   501]  loss 0.027209 (0.038018) train_acc 98.437500 (98.827345)\n",
      "[19,   601]  loss 0.020282 (0.039099) train_acc 100.000000 (98.798877)\n",
      "[19,   701]  loss 0.004264 (0.038654) train_acc 100.000000 (98.805278)\n",
      "[19,   801]  loss 0.032166 (0.038734) train_acc 98.437500 (98.792525)\n",
      "[19,   901]  loss 0.083460 (0.038233) train_acc 96.875000 (98.799945)\n",
      "[20,     1]  loss 0.022649 (0.022649) train_acc 100.000000 (100.000000)\n",
      "[20,   101]  loss 0.025570 (0.033954) train_acc 100.000000 (99.040842)\n",
      "[20,   201]  loss 0.021138 (0.035900) train_acc 100.000000 (98.958333)\n",
      "[20,   301]  loss 0.064977 (0.034963) train_acc 96.875000 (98.951412)\n",
      "[20,   401]  loss 0.073103 (0.034754) train_acc 98.437500 (98.936253)\n",
      "[20,   501]  loss 0.073234 (0.036634) train_acc 96.875000 (98.889721)\n",
      "[20,   601]  loss 0.069359 (0.036972) train_acc 98.437500 (98.866473)\n",
      "[20,   701]  loss 0.056217 (0.037209) train_acc 98.437500 (98.838713)\n",
      "[20,   801]  loss 0.076225 (0.036510) train_acc 96.875000 (98.856898)\n",
      "[20,   901]  loss 0.031458 (0.036846) train_acc 100.000000 (98.839831)\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(qnet, trainloader, cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model after quantization\n",
      "Size (MB): 0.05572\n",
      "Accuracy of the fused and quantized network (trained quantized) on the test images: 98.64% - INT8\n"
     ]
    }
   ],
   "source": [
    "qnet = qnet.cpu()\n",
    "torch.quantization.convert(qnet, inplace=True)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(qnet)\n",
    "\n",
    "score = test(qnet, testloader, cuda=False)\n",
    "print('Accuracy of the fused and quantized network (trained quantized) on the test images: {}% - INT8'.format(score))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1]  loss 2.302431 (2.302431) train_acc 9.375000 (9.375000)\n",
      "[1,   101]  loss 2.292588 (2.298551) train_acc 15.625000 (10.519802)\n",
      "[1,   201]  loss 2.281883 (2.293629) train_acc 21.875000 (14.886505)\n",
      "[1,   301]  loss 2.267573 (2.287043) train_acc 42.187500 (21.496055)\n",
      "[1,   401]  loss 2.238542 (2.278980) train_acc 50.000000 (27.579489)\n",
      "[1,   501]  loss 2.205835 (2.267949) train_acc 65.625000 (33.348927)\n",
      "[1,   601]  loss 2.115737 (2.250990) train_acc 73.437500 (38.971506)\n",
      "[1,   701]  loss 1.959956 (2.223528) train_acc 81.250000 (43.504815)\n",
      "[1,   801]  loss 1.711493 (2.176959) train_acc 81.250000 (47.635768)\n",
      "[1,   901]  loss 1.398292 (2.103770) train_acc 71.875000 (51.068257)\n",
      "[2,     1]  loss 1.058854 (1.058854) train_acc 81.250000 (81.250000)\n",
      "[2,   101]  loss 0.752661 (0.913124) train_acc 85.937500 (82.549505)\n",
      "[2,   201]  loss 0.370132 (0.764782) train_acc 90.625000 (83.426617)\n",
      "[2,   301]  loss 0.411058 (0.677074) train_acc 93.750000 (84.146595)\n",
      "[2,   401]  loss 0.222220 (0.615775) train_acc 93.750000 (85.002338)\n",
      "[2,   501]  loss 0.632270 (0.569673) train_acc 84.375000 (85.797156)\n",
      "[2,   601]  loss 0.259914 (0.531344) train_acc 93.750000 (86.462666)\n",
      "[2,   701]  loss 0.333259 (0.500113) train_acc 87.500000 (87.058666)\n",
      "[2,   801]  loss 0.319613 (0.473651) train_acc 92.187500 (87.542915)\n",
      "[2,   901]  loss 0.224079 (0.449969) train_acc 92.187500 (88.013319)\n",
      "[3,     1]  loss 0.472962 (0.472962) train_acc 87.500000 (87.500000)\n",
      "[3,   101]  loss 0.276838 (0.257404) train_acc 89.062500 (92.094678)\n",
      "[3,   201]  loss 0.275754 (0.245334) train_acc 90.625000 (92.529540)\n",
      "[3,   301]  loss 0.143189 (0.241308) train_acc 93.750000 (92.519726)\n",
      "[3,   401]  loss 0.120525 (0.232121) train_acc 95.312500 (92.787562)\n",
      "[3,   501]  loss 0.156372 (0.228769) train_acc 92.187500 (92.873628)\n",
      "[3,   601]  loss 0.178699 (0.222286) train_acc 93.750000 (93.076643)\n",
      "[3,   701]  loss 0.316785 (0.217233) train_acc 89.062500 (93.255171)\n",
      "[3,   801]  loss 0.132726 (0.213752) train_acc 92.187500 (93.389123)\n",
      "[3,   901]  loss 0.114739 (0.209039) train_acc 98.437500 (93.529759)\n",
      "[4,     1]  loss 0.326593 (0.326593) train_acc 89.062500 (89.062500)\n",
      "[4,   101]  loss 0.163235 (0.159768) train_acc 95.312500 (95.111386)\n",
      "[4,   201]  loss 0.255929 (0.165413) train_acc 92.187500 (94.993781)\n",
      "[4,   301]  loss 0.195131 (0.157073) train_acc 95.312500 (95.343646)\n",
      "[4,   401]  loss 0.124966 (0.154417) train_acc 95.312500 (95.382637)\n",
      "[4,   501]  loss 0.073346 (0.153835) train_acc 100.000000 (95.377994)\n",
      "[4,   601]  loss 0.049656 (0.151338) train_acc 100.000000 (95.437292)\n",
      "[4,   701]  loss 0.125417 (0.150618) train_acc 98.437500 (95.481901)\n",
      "[4,   801]  loss 0.216268 (0.150590) train_acc 93.750000 (95.470506)\n",
      "[4,   901]  loss 0.099546 (0.149159) train_acc 96.875000 (95.498058)\n",
      "[5,     1]  loss 0.081762 (0.081762) train_acc 98.437500 (98.437500)\n",
      "[5,   101]  loss 0.034126 (0.122987) train_acc 100.000000 (96.240718)\n",
      "[5,   201]  loss 0.111345 (0.127344) train_acc 95.312500 (96.120958)\n",
      "[5,   301]  loss 0.081637 (0.126567) train_acc 96.875000 (96.163829)\n",
      "[5,   401]  loss 0.073691 (0.127606) train_acc 96.875000 (96.146353)\n",
      "[5,   501]  loss 0.131243 (0.124049) train_acc 93.750000 (96.251248)\n",
      "[5,   601]  loss 0.110003 (0.123334) train_acc 96.875000 (96.303037)\n",
      "[5,   701]  loss 0.032414 (0.121567) train_acc 100.000000 (96.346737)\n",
      "[5,   801]  loss 0.066116 (0.120126) train_acc 96.875000 (96.354167)\n",
      "[5,   901]  loss 0.134999 (0.119327) train_acc 96.875000 (96.385960)\n",
      "[6,     1]  loss 0.090722 (0.090722) train_acc 95.312500 (95.312500)\n",
      "[6,   101]  loss 0.094654 (0.104907) train_acc 95.312500 (96.936881)\n",
      "[6,   201]  loss 0.070083 (0.103324) train_acc 95.312500 (96.937189)\n",
      "[6,   301]  loss 0.032446 (0.102371) train_acc 100.000000 (96.880191)\n",
      "[6,   401]  loss 0.102223 (0.102861) train_acc 96.875000 (96.913965)\n",
      "[6,   501]  loss 0.018278 (0.105369) train_acc 100.000000 (96.853169)\n",
      "[6,   601]  loss 0.037103 (0.103690) train_acc 100.000000 (96.887999)\n",
      "[6,   701]  loss 0.070735 (0.102384) train_acc 98.437500 (96.937411)\n",
      "[6,   801]  loss 0.233470 (0.102716) train_acc 90.625000 (96.892556)\n",
      "[6,   901]  loss 0.142502 (0.102766) train_acc 95.312500 (96.899279)\n",
      "[7,     1]  loss 0.031148 (0.031148) train_acc 100.000000 (100.000000)\n",
      "[7,   101]  loss 0.076631 (0.101636) train_acc 96.875000 (96.983292)\n",
      "[7,   201]  loss 0.132508 (0.098339) train_acc 96.875000 (97.077114)\n",
      "[7,   301]  loss 0.044016 (0.096518) train_acc 100.000000 (97.176080)\n",
      "[7,   401]  loss 0.066238 (0.096095) train_acc 96.875000 (97.132170)\n",
      "[7,   501]  loss 0.144187 (0.094459) train_acc 96.875000 (97.143214)\n",
      "[7,   601]  loss 0.062752 (0.094334) train_acc 96.875000 (97.155782)\n",
      "[7,   701]  loss 0.057442 (0.092829) train_acc 98.437500 (97.198199)\n",
      "[7,   801]  loss 0.156645 (0.092160) train_acc 95.312500 (97.243680)\n",
      "[7,   901]  loss 0.029702 (0.091418) train_acc 100.000000 (97.242647)\n",
      "[8,     1]  loss 0.112592 (0.112592) train_acc 95.312500 (95.312500)\n",
      "[8,   101]  loss 0.022678 (0.082780) train_acc 100.000000 (97.199876)\n",
      "[8,   201]  loss 0.119630 (0.081766) train_acc 96.875000 (97.380286)\n",
      "[8,   301]  loss 0.035784 (0.083677) train_acc 100.000000 (97.414867)\n",
      "[8,   401]  loss 0.080387 (0.084190) train_acc 96.875000 (97.404925)\n",
      "[8,   501]  loss 0.104168 (0.084790) train_acc 98.437500 (97.414546)\n",
      "[8,   601]  loss 0.059250 (0.084545) train_acc 98.437500 (97.452163)\n",
      "[8,   701]  loss 0.016772 (0.083733) train_acc 100.000000 (97.481277)\n",
      "[8,   801]  loss 0.059754 (0.083200) train_acc 98.437500 (97.493368)\n",
      "[8,   901]  loss 0.069375 (0.082415) train_acc 96.875000 (97.507977)\n",
      "[9,     1]  loss 0.150074 (0.150074) train_acc 96.875000 (96.875000)\n",
      "[9,   101]  loss 0.093763 (0.081377) train_acc 98.437500 (97.679455)\n",
      "[9,   201]  loss 0.084682 (0.080499) train_acc 95.312500 (97.481343)\n",
      "[9,   301]  loss 0.041656 (0.078033) train_acc 100.000000 (97.591362)\n",
      "[9,   401]  loss 0.054863 (0.079599) train_acc 98.437500 (97.599751)\n",
      "[9,   501]  loss 0.101078 (0.078808) train_acc 96.875000 (97.642216)\n",
      "[9,   601]  loss 0.058062 (0.077314) train_acc 98.437500 (97.680948)\n",
      "[9,   701]  loss 0.031150 (0.077653) train_acc 98.437500 (97.635075)\n",
      "[9,   801]  loss 0.016732 (0.077174) train_acc 100.000000 (97.635768)\n",
      "[9,   901]  loss 0.082416 (0.076837) train_acc 96.875000 (97.664054)\n",
      "[10,     1]  loss 0.087738 (0.087738) train_acc 96.875000 (96.875000)\n",
      "[10,   101]  loss 0.049862 (0.073102) train_acc 100.000000 (97.880569)\n",
      "[10,   201]  loss 0.034947 (0.071739) train_acc 98.437500 (97.753420)\n",
      "[10,   301]  loss 0.081134 (0.071633) train_acc 98.437500 (97.762666)\n",
      "[10,   401]  loss 0.044689 (0.073254) train_acc 98.437500 (97.708853)\n",
      "[10,   501]  loss 0.031404 (0.071465) train_acc 98.437500 (97.776322)\n",
      "[10,   601]  loss 0.117471 (0.071417) train_acc 96.875000 (97.800541)\n",
      "[10,   701]  loss 0.011601 (0.071323) train_acc 100.000000 (97.806705)\n",
      "[10,   801]  loss 0.020799 (0.071274) train_acc 100.000000 (97.807428)\n",
      "[10,   901]  loss 0.068368 (0.070698) train_acc 96.875000 (97.816662)\n",
      "[11,     1]  loss 0.041186 (0.041186) train_acc 100.000000 (100.000000)\n",
      "[11,   101]  loss 0.085316 (0.067797) train_acc 93.750000 (97.988861)\n",
      "[11,   201]  loss 0.111807 (0.069217) train_acc 98.437500 (97.885572)\n",
      "[11,   301]  loss 0.018589 (0.066679) train_acc 100.000000 (97.939161)\n",
      "[11,   401]  loss 0.015477 (0.067753) train_acc 100.000000 (97.860817)\n",
      "[11,   501]  loss 0.081690 (0.066375) train_acc 96.875000 (97.932260)\n",
      "[11,   601]  loss 0.092210 (0.066338) train_acc 98.437500 (97.909734)\n",
      "[11,   701]  loss 0.139904 (0.065970) train_acc 95.312500 (97.924840)\n",
      "[11,   801]  loss 0.044525 (0.066195) train_acc 96.875000 (97.926420)\n",
      "[11,   901]  loss 0.076784 (0.066632) train_acc 95.312500 (97.906840)\n",
      "[12,     1]  loss 0.069206 (0.069206) train_acc 98.437500 (98.437500)\n",
      "[12,   101]  loss 0.075755 (0.066515) train_acc 98.437500 (97.834158)\n",
      "[12,   201]  loss 0.062947 (0.067424) train_acc 96.875000 (97.854478)\n",
      "[12,   301]  loss 0.031476 (0.065064) train_acc 100.000000 (97.980689)\n",
      "[12,   401]  loss 0.081509 (0.064333) train_acc 95.312500 (98.001091)\n",
      "[12,   501]  loss 0.027964 (0.063689) train_acc 98.437500 (98.016467)\n",
      "[12,   601]  loss 0.042407 (0.063858) train_acc 98.437500 (98.003328)\n",
      "[12,   701]  loss 0.221457 (0.063679) train_acc 92.187500 (97.991708)\n",
      "[12,   801]  loss 0.080284 (0.064381) train_acc 98.437500 (97.981039)\n",
      "[12,   901]  loss 0.121937 (0.063579) train_acc 95.312500 (98.026498)\n",
      "[13,     1]  loss 0.050933 (0.050933) train_acc 98.437500 (98.437500)\n",
      "[13,   101]  loss 0.074990 (0.057492) train_acc 96.875000 (98.189975)\n",
      "[13,   201]  loss 0.067411 (0.059647) train_acc 96.875000 (98.243159)\n",
      "[13,   301]  loss 0.138623 (0.056838) train_acc 96.875000 (98.349252)\n",
      "[13,   401]  loss 0.048888 (0.057406) train_acc 98.437500 (98.273847)\n",
      "[13,   501]  loss 0.008390 (0.058206) train_acc 100.000000 (98.225424)\n",
      "[13,   601]  loss 0.075555 (0.058723) train_acc 98.437500 (98.198315)\n",
      "[13,   701]  loss 0.032395 (0.059393) train_acc 98.437500 (98.174483)\n",
      "[13,   801]  loss 0.012989 (0.059523) train_acc 100.000000 (98.172207)\n",
      "[13,   901]  loss 0.043719 (0.059607) train_acc 98.437500 (98.161765)\n",
      "[14,     1]  loss 0.047825 (0.047825) train_acc 98.437500 (98.437500)\n",
      "[14,   101]  loss 0.080831 (0.057211) train_acc 98.437500 (98.236386)\n",
      "[14,   201]  loss 0.017666 (0.057222) train_acc 100.000000 (98.204291)\n",
      "[14,   301]  loss 0.093828 (0.059726) train_acc 96.875000 (98.167566)\n",
      "[14,   401]  loss 0.015623 (0.057013) train_acc 100.000000 (98.230985)\n",
      "[14,   501]  loss 0.135346 (0.056451) train_acc 95.312500 (98.262849)\n",
      "[14,   601]  loss 0.063987 (0.056809) train_acc 98.437500 (98.258111)\n",
      "[14,   701]  loss 0.006335 (0.056984) train_acc 100.000000 (98.274786)\n",
      "[14,   801]  loss 0.012905 (0.058114) train_acc 100.000000 (98.236579)\n",
      "[14,   901]  loss 0.028713 (0.057918) train_acc 100.000000 (98.245006)\n",
      "[15,     1]  loss 0.065171 (0.065171) train_acc 98.437500 (98.437500)\n",
      "[15,   101]  loss 0.039870 (0.054273) train_acc 98.437500 (98.468441)\n",
      "[15,   201]  loss 0.064553 (0.054727) train_acc 96.875000 (98.507463)\n",
      "[15,   301]  loss 0.013498 (0.054719) train_acc 100.000000 (98.437500)\n",
      "[15,   401]  loss 0.028719 (0.054595) train_acc 100.000000 (98.394638)\n",
      "[15,   501]  loss 0.049873 (0.054400) train_acc 98.437500 (98.412550)\n",
      "[15,   601]  loss 0.034541 (0.054706) train_acc 98.437500 (98.393303)\n",
      "[15,   701]  loss 0.020291 (0.054827) train_acc 100.000000 (98.377318)\n",
      "[15,   801]  loss 0.021402 (0.054504) train_acc 100.000000 (98.384831)\n",
      "[15,   901]  loss 0.047024 (0.054643) train_acc 98.437500 (98.368133)\n",
      "[16,     1]  loss 0.098678 (0.098678) train_acc 95.312500 (95.312500)\n",
      "[16,   101]  loss 0.031282 (0.051252) train_acc 100.000000 (98.576733)\n",
      "[16,   201]  loss 0.064869 (0.051658) train_acc 96.875000 (98.453047)\n",
      "[16,   301]  loss 0.013993 (0.049602) train_acc 100.000000 (98.499792)\n",
      "[16,   401]  loss 0.013383 (0.049902) train_acc 100.000000 (98.449190)\n",
      "[16,   501]  loss 0.029003 (0.051428) train_acc 100.000000 (98.403194)\n",
      "[16,   601]  loss 0.146842 (0.051524) train_acc 93.750000 (98.427101)\n",
      "[16,   701]  loss 0.099790 (0.051962) train_acc 96.875000 (98.426355)\n",
      "[16,   801]  loss 0.053109 (0.051700) train_acc 96.875000 (98.443352)\n",
      "[16,   901]  loss 0.109748 (0.052505) train_acc 96.875000 (98.413221)\n",
      "[17,     1]  loss 0.006021 (0.006021) train_acc 100.000000 (100.000000)\n",
      "[17,   101]  loss 0.053695 (0.054055) train_acc 98.437500 (98.298267)\n",
      "[17,   201]  loss 0.183139 (0.052307) train_acc 96.875000 (98.445274)\n",
      "[17,   301]  loss 0.037757 (0.053936) train_acc 96.875000 (98.395972)\n",
      "[17,   401]  loss 0.147631 (0.053682) train_acc 98.437500 (98.398535)\n",
      "[17,   501]  loss 0.057257 (0.051021) train_acc 96.875000 (98.465569)\n",
      "[17,   601]  loss 0.063990 (0.050603) train_acc 96.875000 (98.471298)\n",
      "[17,   701]  loss 0.028885 (0.050710) train_acc 98.437500 (98.437500)\n",
      "[17,   801]  loss 0.028530 (0.050867) train_acc 100.000000 (98.427747)\n",
      "[17,   901]  loss 0.087951 (0.050477) train_acc 95.312500 (98.447905)\n",
      "[18,     1]  loss 0.022306 (0.022306) train_acc 98.437500 (98.437500)\n",
      "[18,   101]  loss 0.025378 (0.047545) train_acc 98.437500 (98.607673)\n",
      "[18,   201]  loss 0.010526 (0.048932) train_acc 100.000000 (98.538557)\n",
      "[18,   301]  loss 0.017172 (0.048451) train_acc 100.000000 (98.530939)\n",
      "[18,   401]  loss 0.039836 (0.048178) train_acc 96.875000 (98.550499)\n",
      "[18,   501]  loss 0.066200 (0.048710) train_acc 96.875000 (98.534182)\n",
      "[18,   601]  loss 0.078210 (0.048079) train_acc 98.437500 (98.549293)\n",
      "[18,   701]  loss 0.034566 (0.049436) train_acc 100.000000 (98.508827)\n",
      "[18,   801]  loss 0.030105 (0.048795) train_acc 100.000000 (98.525281)\n",
      "[18,   901]  loss 0.106699 (0.048454) train_acc 98.437500 (98.538083)\n",
      "[19,     1]  loss 0.004582 (0.004582) train_acc 100.000000 (100.000000)\n",
      "[19,   101]  loss 0.023206 (0.044193) train_acc 100.000000 (98.886139)\n",
      "[19,   201]  loss 0.100665 (0.049601) train_acc 96.875000 (98.569652)\n",
      "[19,   301]  loss 0.049391 (0.046393) train_acc 98.437500 (98.624377)\n",
      "[19,   401]  loss 0.033586 (0.046535) train_acc 98.437500 (98.632325)\n",
      "[19,   501]  loss 0.083860 (0.047395) train_acc 95.312500 (98.587201)\n",
      "[19,   601]  loss 0.054868 (0.047736) train_acc 96.875000 (98.583091)\n",
      "[19,   701]  loss 0.018360 (0.047927) train_acc 98.437500 (98.593527)\n",
      "[19,   801]  loss 0.031777 (0.047936) train_acc 98.437500 (98.603308)\n",
      "[19,   901]  loss 0.115403 (0.047334) train_acc 96.875000 (98.621324)\n",
      "[20,     1]  loss 0.040285 (0.040285) train_acc 98.437500 (98.437500)\n",
      "[20,   101]  loss 0.064714 (0.045780) train_acc 98.437500 (98.576733)\n",
      "[20,   201]  loss 0.055434 (0.044060) train_acc 96.875000 (98.647388)\n",
      "[20,   301]  loss 0.038597 (0.047518) train_acc 98.437500 (98.556894)\n",
      "[20,   401]  loss 0.011687 (0.046223) train_acc 100.000000 (98.624532)\n",
      "[20,   501]  loss 0.015771 (0.045833) train_acc 100.000000 (98.646457)\n",
      "[20,   601]  loss 0.011476 (0.045036) train_acc 100.000000 (98.679285)\n",
      "[20,   701]  loss 0.009557 (0.045676) train_acc 100.000000 (98.653709)\n",
      "[20,   801]  loss 0.007909 (0.045452) train_acc 100.000000 (98.652076)\n",
      "[20,   901]  loss 0.023107 (0.045585) train_acc 100.000000 (98.616121)\n",
      "Finished Training\n",
      "Size of model after quantization\n",
      "Size (MB): 0.05572\n",
      "Accuracy of the fused and quantized network (trained quantized) on the test images: 98.54% - INT8\n"
     ]
    }
   ],
   "source": [
    "qnet = Net(q=True)\n",
    "fuse_modules(qnet)\n",
    "qnet.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(qnet, inplace=True)\n",
    "qnet = qnet.cuda()\n",
    "train(qnet, trainloader, cuda=True, q=True)\n",
    "qnet = qnet.cpu()\n",
    "torch.quantization.convert(qnet, inplace=True)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(qnet)\n",
    "\n",
    "score = test(qnet, testloader, cuda=False)\n",
    "print('Accuracy of the fused and quantized network (trained quantized) on the test images: {}% - INT8'.format(score))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input tensor\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "# qconfig = get_default_qconfig(\"fbgemm\")  # Use \"qnnpack\" for ARM devices\n",
    "# prepared_model = prepare_fx(model, {\"\": qconfig}, dummy_input)\n",
    "# quantized_model_fx = convert_fx(prepared_model)\n",
    "print(qnet)\n",
    "# Export the quantized model to ONNX\n",
    "torch.onnx.export(\n",
    "    qnet,\n",
    "    dummy_input,\n",
    "    \"qat_model_mnist.onnx\",\n",
    "    opset_version=13,  # Ensure compatibility with INT8 ops\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv_pytrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

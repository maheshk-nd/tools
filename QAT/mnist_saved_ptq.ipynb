{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226799/1387068913.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"mnist_cnn.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "state_dict = torch.load(\"mnist_cnn.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import quant_modules\n",
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='histogram')\n",
    "quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_226799/523587435.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"mnist_cnn.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): QuantConv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (conv2): QuantConv2d(\n",
       "    32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=3136, out_features=128, bias=True\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat = CNN().to(device)\n",
    "\n",
    "state_dict = torch.load(\"mnist_cnn.pth\")\n",
    "model_qat.load_state_dict(state_dict)\n",
    "model_qat.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.33s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0203 11:58:25.454816 139764016534784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 11:58:25.455624 139764016534784 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 11:58:25.456214 139764016534784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 11:58:25.456840 139764016534784 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 11:58:25.457414 139764016534784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 11:58:25.458050 139764016534784 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 11:58:25.458580 139764016534784 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 11:58:25.459197 139764016534784 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 11:58:25.463215 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 11:58:25.463669 139764016534784 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0203 11:58:25.464229 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0203 11:58:25.465682 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 11:58:25.466220 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0203 11:58:25.467229 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 11:58:25.467765 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([128, 1]).\n",
      "W0203 11:58:25.471077 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 11:58:25.472745 139764016534784 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.8201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "conv1._weight_quantizer                 : TensorQuantizer(8bit fake axis=0 amax=[0.3197, 0.5787](32) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "conv2._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=4.4838 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "conv2._weight_quantizer                 : TensorQuantizer(8bit fake axis=0 amax=[0.1594, 0.4042](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "fc1._input_quantizer                    : TensorQuantizer(8bit fake per-tensor amax=8.7644 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "fc1._weight_quantizer                   : TensorQuantizer(8bit fake axis=0 amax=[0.0230, 0.2383](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "fc2._input_quantizer                    : TensorQuantizer(8bit fake per-tensor amax=44.7928 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "fc2._weight_quantizer                   : TensorQuantizer(8bit fake axis=0 amax=[0.1774, 0.2029](10) calibrator=MaxCalibrator scale=1.0 quant)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    model.cuda()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    collect_stats(model_qat, train_loader, num_batches=2)\n",
    "    compute_amax(model_qat, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0229, Accuracy: 99.34%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}, Accuracy: {correct / len(test_loader.dataset) * 100:.2f}%\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    evaluate(model_qat, device, test_loader, criterion)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model_qat.state_dict(), \"mnist_quant-calibrated.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0203 12:00:03.983517 139764016534784 tensor_quantizer.py:281] Use Pytorch's native experimental fake quantization.\n",
      "/home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:284: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if amax.numel() == 1:\n",
      "/home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:286: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  inputs, amax.item() / bound, 0,\n",
      "/home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:292: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch IR graph at exception: graph(%inputs.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0),\n",
      "      %conv1.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %conv1.bias : Float(32, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %conv1._input_quantizer._amax : Float(requires_grad=0, device=cuda:0),\n",
      "      %conv1._weight_quantizer._amax : Float(32, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %conv2.weight : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cuda:0),\n",
      "      %conv2.bias : Float(64, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %conv2._input_quantizer._amax : Float(requires_grad=0, device=cuda:0),\n",
      "      %conv2._weight_quantizer._amax : Float(64, 1, 1, 1, strides=[1, 1, 1, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc1.weight : Float(128, 3136, strides=[3136, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc1.bias : Float(128, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc1._input_quantizer._amax : Float(requires_grad=0, device=cuda:0),\n",
      "      %fc1._weight_quantizer._amax : Float(128, 1, strides=[1, 1], requires_grad=0, device=cuda:0),\n",
      "      %fc2.weight : Float(10, 128, strides=[128, 1], requires_grad=1, device=cuda:0),\n",
      "      %fc2.bias : Float(10, strides=[1], requires_grad=1, device=cuda:0),\n",
      "      %fc2._input_quantizer._amax : Float(requires_grad=0, device=cuda:0),\n",
      "      %fc2._weight_quantizer._amax : Float(10, 1, strides=[1, 1], requires_grad=0, device=cuda:0)):\n",
      "  %1003 : Double(device=cpu) = prim::Constant[value={0.0222056}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %1004 : Long(device=cpu) = prim::Constant[value={0}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %1005 : Long(device=cpu) = prim::Constant[value={-128}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %1006 : Long(device=cpu) = prim::Constant[value={127}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %736 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::fake_quantize_per_tensor_affine(%inputs.1, %1003, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285:0\n",
      "  %770 : Float(32, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %771 : Int(32, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %775 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_channel_affine(%conv1.weight, %770, %771, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %986 : int[] = prim::Constant[value=[1, 1]]()\n",
      "  %1007 : Bool(device=cpu) = prim::Constant[value={0}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1\n",
      "  %989 : int[] = prim::Constant[value=[0, 0]]()\n",
      "  %1008 : Long(device=cpu) = prim::Constant[value={1}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1\n",
      "  %1009 : Bool(device=cpu) = prim::Constant[value={1}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1\n",
      "  %input.1 : Float(1, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%736, %775, %conv1.bias, %986, %986, %986, %1007, %989, %1008, %1007, %1007, %1009, %1009), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv1 # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/quant_conv.py:130:0\n",
      "  %795 : Float(1, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.1), scope: __main__.CNN::/torch.nn.modules.activation.ReLU::relu # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/nn/functional.py:1704:0\n",
      "  %990 : int[] = prim::Constant[value=[2, 2]]()\n",
      "  %inputs.5 : Float(1, 32, 14, 14, strides=[6272, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%795, %990, %990, %989, %986, %1007), scope: __main__.CNN::/torch.nn.modules.pooling.MaxPool2d::pool # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/nn/functional.py:830:0\n",
      "  %1010 : Double(device=cpu) = prim::Constant[value={0.0353055}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %818 : Float(1, 32, 14, 14, strides=[6272, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_tensor_affine(%inputs.5, %1010, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285:0\n",
      "  %852 : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %853 : Int(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %857 : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_channel_affine(%conv2.weight, %852, %853, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %input.3 : Float(1, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=0, device=cuda:0) = aten::_convolution(%818, %857, %conv2.bias, %986, %986, %986, %1007, %989, %1008, %1007, %1007, %1009, %1009), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_conv.QuantConv2d::conv2 # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/quant_conv.py:130:0\n",
      "  %877 : Float(1, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.3), scope: __main__.CNN::/torch.nn.modules.activation.ReLU::relu # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/nn/functional.py:1704:0\n",
      "  %891 : Float(1, 64, 7, 7, strides=[3136, 49, 7, 1], requires_grad=1, device=cuda:0) = aten::max_pool2d(%877, %990, %990, %989, %986, %1007), scope: __main__.CNN::/torch.nn.modules.pooling.MaxPool2d::pool # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/nn/functional.py:830:0\n",
      "  %1002 : int[] = prim::Constant[value=[-1, 3136]]()\n",
      "  %inputs.9 : Float(1, 3136, strides=[3136, 1], requires_grad=1, device=cuda:0) = aten::view(%891, %1002), scope: __main__.CNN:: # /tmp/ipykernel_226799/1026670601.py:16:0\n",
      "  %1011 : Double(device=cpu) = prim::Constant[value={0.0690109}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %904 : Float(1, 3136, strides=[3136, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_tensor_affine(%inputs.9, %1011, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285:0\n",
      "  %932 : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %933 : Int(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %937 : Float(128, 3136, strides=[3136, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_channel_affine(%fc1.weight, %932, %933, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc1/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %input.5 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::linear(%904, %937, %fc1.bias), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc1 # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/quant_linear.py:73:0\n",
      "  %input : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::relu(%input.5), scope: __main__.CNN::/torch.nn.modules.activation.ReLU::relu # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/nn/functional.py:1704:0\n",
      "  %1012 : Double(device=cpu) = prim::Constant[value={0.5}](), scope: __main__.CNN::/torch.nn.modules.dropout.Dropout::dropout\n",
      "  %inputs.13 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::dropout(%input, %1012, %1007), scope: __main__.CNN::/torch.nn.modules.dropout.Dropout::dropout # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/nn/functional.py:1425:0\n",
      "  %1013 : Double(device=cpu) = prim::Constant[value={0.3527}](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer\n",
      "  %951 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_tensor_affine(%inputs.13, %1013, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_input_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285:0\n",
      "  %979 : Float(10, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=0.001 *  1.5581  1.5980  1.5914  1.4553  1.3967  1.4701  1.4982  1.5879  1.4620  1.5141 [ CUDAFloatType{10} ]](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %980 : Int(10, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value= 0  0  0  0  0  0  0  0  0  0 [ CUDAIntType{10} ]](), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %984 : Float(10, 128, strides=[128, 1], requires_grad=1, device=cuda:0) = aten::fake_quantize_per_channel_affine(%fc2.weight, %979, %980, %1004, %1005, %1006), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc2/pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer::_weight_quantizer # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:294:0\n",
      "  %985 : Float(1, 10, strides=[10, 1], requires_grad=1, device=cuda:0) = aten::linear(%951, %984, %fc2.bias), scope: __main__.CNN::/pytorch_quantization.nn.modules.quant_linear.QuantLinear::fc2 # /home/ubuntu/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/pytorch_quantization/nn/modules/quant_linear.py:73:0\n",
      "  return (%985)\n",
      "\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::fake_quantize_per_channel_affine' to ONNX opset version 10 is not supported. Support for this operator was added in version 13, try exporting with this version.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m quant_nn\u001b[38;5;241m.\u001b[39mTensorQuantizer\u001b[38;5;241m.\u001b[39muse_fb_fake_quant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# enable_onnx_checker needs to be disabled. See notes below.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist_quant.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_onnx_checker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactual_input_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/onnx/__init__.py:375\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, input_names, output_names, opset_version, dynamic_axes, keep_initializers_as_inputs, dynamo, external_data, dynamic_shapes, report, verify, profile, dump_exported_program, artifacts_dir, fallback, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining, **_)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_shapes:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe exporter only supports dynamic shapes \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrough parameter dynamic_axes when dynamo=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n\u001b[0;32m--> 375\u001b[0m \u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/onnx/utils.py:502\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;241m+\u001b[39m (kwargs,)\n\u001b[0;32m--> 502\u001b[0m \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/onnx/utils.py:1564\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1562\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1564\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1578\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1579\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1580\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/onnx/utils.py:1117\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1128\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/onnx/utils.py:639\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    636\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    637\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 639\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    641\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/conda_venv_pytrt/lib/python3.12/site-packages/torch/onnx/utils.py:1848\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, values_in_env, new_nodes, operator_export_type)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1843\u001b[0m         \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m graph_context\u001b[38;5;241m.\u001b[39mop(\n\u001b[1;32m   1845\u001b[0m             op_name, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattrs, outputs\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39moutputsSize()\n\u001b[1;32m   1846\u001b[0m         )  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1849\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1850\u001b[0m         opset_version,\n\u001b[1;32m   1851\u001b[0m         symbolic_function_group\u001b[38;5;241m.\u001b[39mget_min_supported()\n\u001b[1;32m   1852\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1853\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1854\u001b[0m     )\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m operator_export_type \u001b[38;5;241m==\u001b[39m _C_onnx\u001b[38;5;241m.\u001b[39mOperatorExportTypes\u001b[38;5;241m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::fake_quantize_per_channel_affine' to ONNX opset version 10 is not supported. Support for this operator was added in version 13, try exporting with this version."
     ]
    }
   ],
   "source": [
    "import pytorch_quantization\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device='cuda')\n",
    "\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "#with pytorch_quantization.enable_onnx_export():#\n",
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "# enable_onnx_checker needs to be disabled. See notes below.\n",
    "torch.onnx.export(\n",
    "    model_qat, dummy_input, \"mnist_quant.onnx\", verbose=True, opset_version=10, enable_onnx_checker=False, input_names = [ \"actual_input_1\" ], output_names = [ \"output1\" ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv_pytrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

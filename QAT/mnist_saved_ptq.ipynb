{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_329517/1387068913.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"mnist_cnn.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "state_dict = torch.load(\"mnist_cnn.pth\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_quantization import quant_modules\n",
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_desc_input = QuantDescriptor(calib_method='histogram')\n",
    "quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_329517/523587435.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"mnist_cnn.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): QuantConv2d(\n",
       "    1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (conv2): QuantConv2d(\n",
       "    32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=3136, out_features=128, bias=True\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (fc2): QuantLinear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=HistogramCalibrator scale=1.0 quant)\n",
       "    (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat = CNN().to(device)\n",
    "\n",
    "state_dict = torch.load(\"mnist_cnn.pth\")\n",
    "model_qat.load_state_dict(state_dict)\n",
    "model_qat.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.28s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0203 06:16:29.298797 139736912962816 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 06:16:29.299856 139736912962816 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 06:16:29.300999 139736912962816 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 06:16:29.301458 139736912962816 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 06:16:29.302242 139736912962816 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 06:16:29.302683 139736912962816 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 06:16:29.303240 139736912962816 tensor_quantizer.py:174] Disable HistogramCalibrator\n",
      "W0203 06:16:29.303801 139736912962816 tensor_quantizer.py:174] Disable MaxCalibrator\n",
      "W0203 06:16:29.317455 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 06:16:29.318041 139736912962816 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0203 06:16:29.318578 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([32, 1, 1, 1]).\n",
      "W0203 06:16:29.321136 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 06:16:29.321746 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0203 06:16:29.323002 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 06:16:29.323844 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([128, 1]).\n",
      "W0203 06:16:29.324780 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).\n",
      "W0203 06:16:29.325304 139736912962816 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([10, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=2.8201 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "conv1._weight_quantizer                 : TensorQuantizer(8bit fake axis=0 amax=[0.3197, 0.5787](32) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "conv2._input_quantizer                  : TensorQuantizer(8bit fake per-tensor amax=4.4820 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "conv2._weight_quantizer                 : TensorQuantizer(8bit fake axis=0 amax=[0.1594, 0.4042](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "fc1._input_quantizer                    : TensorQuantizer(8bit fake per-tensor amax=8.7409 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "fc1._weight_quantizer                   : TensorQuantizer(8bit fake axis=0 amax=[0.0230, 0.2383](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "fc2._input_quantizer                    : TensorQuantizer(8bit fake per-tensor amax=44.7653 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "fc2._weight_quantizer                   : TensorQuantizer(8bit fake axis=0 amax=[0.1774, 0.2029](10) calibrator=MaxCalibrator scale=1.0 quant)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    model.cuda()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    collect_stats(model_qat, train_loader, num_batches=2)\n",
    "    compute_amax(model_qat, method=\"percentile\", percentile=99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0229, Accuracy: 99.33%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader):.4f}, Accuracy: {correct / len(test_loader.dataset) * 100:.2f}%\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    evaluate(model_qat, device, test_loader, criterion)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model_qat.state_dict(), \"mnist_quant-calibrated.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_quantization' has no attribute 'enable_onnx_export'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_names \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_input_1\u001b[39m\u001b[38;5;124m\"\u001b[39m ]\n\u001b[1;32m      5\u001b[0m output_names \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput1\u001b[39m\u001b[38;5;124m\"\u001b[39m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpytorch_quantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_onnx_export\u001b[49m():\n\u001b[1;32m      8\u001b[0m      \u001b[38;5;66;03m# enable_onnx_checker needs to be disabled. See notes below.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m      torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(\n\u001b[1;32m     10\u001b[0m          model_qat, dummy_input, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_quant.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, opset_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, enable_onnx_checker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_names \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_input_1\u001b[39m\u001b[38;5;124m\"\u001b[39m ], output_names \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput1\u001b[39m\u001b[38;5;124m\"\u001b[39m ]\n\u001b[1;32m     11\u001b[0m          )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pytorch_quantization' has no attribute 'enable_onnx_export'"
     ]
    }
   ],
   "source": [
    "import pytorch_quantization\n",
    "dummy_input = torch.randn(1, 1, 28, 28, device='cuda')\n",
    "\n",
    "input_names = [ \"actual_input_1\" ]\n",
    "output_names = [ \"output1\" ]\n",
    "\n",
    "with pytorch_quantization.enable_onnx_export():\n",
    "     # enable_onnx_checker needs to be disabled. See notes below.\n",
    "     torch.onnx.export(\n",
    "         model_qat, dummy_input, \"mnist_quant.onnx\", verbose=True, opset_version=10, enable_onnx_checker=False, input_names = [ \"actual_input_1\" ], output_names = [ \"output1\" ]\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv_pytrt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
